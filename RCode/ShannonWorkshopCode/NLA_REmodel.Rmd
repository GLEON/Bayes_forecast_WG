Spatial random effects and NLA data. 
for GLEON Workshop Dec 1, 2017
author: "Shannon LaDeau"
date: "November 2017"

---

```{r}
rm(list = ls())  #remove anything stored in workspace from previous exercises

#install.packages("tidyverse")
#library(readxl)
library(rjags)
```


Data used are from the US National Lakes Assessment: https://www.epa.gov/national-aquatic-resource-surveys/nla
The focal response variable for this example is conductivity - an estimate of total dissolved solids in the water.

```{r}
setwd("~/Documents/GLEON/Bayes Workshop/DATA/NLA")
dat = read.csv("~/Documents/GLEON/Bayes Workshop/DATA/NLA/GLEON NLA data.csv", header=T)

y = dat$COND_US  #focal response variable
region= dat$OMER_L3_ECO  #ecological region indicator
ph=dat#PH
hist(y)
range(y,na.rm=T)
length(y)

```

Look at the data y. Below I proceed with a lognormal assumption - but that isn't the only option. Note that there are also missing condutance measurements.


##The Simple Model
As a first approach, consider the simple case of finding the mean and variance in log conductance across all measured lakes.


```{r}
NormMean <- "
model {
  mu ~ dnorm(mu0,T) # prior on the mean 
  S ~ dgamma(a_obs,r_obs)
  for(i in 1:N){
    y[i] ~ dnorm(mu,S) # data model
  }
}
"
```

### Enter data
We assume a normal prior on the mean (of log y) with mean 7.2 and variance 1.5 - this should be from a different dataset than what is used here): 

```{r}
  ## prior standard deviation
sigma=1.5
data = list(N=length(y),y = log(y), ## data
            mu0 = 7.2, ## prior mean
            T = 1/sigma^2, ## prior precision
          a_obs = 0.01, r_obs= 0.01 )## prior mean precision 
```

### Initial conditions: 

Here we resample the real data to generate some reasonable initial conditions for the unknown precision (S) and posterior mean conductance (mu). 

```{r}
nchain = 3
inits <- list()
for(i in 1:nchain){
  y.samp = sample(log(y),length(y),replace=TRUE)
  inits[[i]] <- list(S=1/var(y.samp), mu=mean(y.samp,na.rm=T))
}
```


### Running JAGS


```{r}
j.model   <- jags.model (file = textConnection(NormMean),
                             data = data,
                             inits = inits,
                             n.chains = 3)

```

Look at the number of unobserved stochastic nodes. Why is this more than the 2 we specified initial values for?

```{r}
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu","S"),
                                n.iter = 1000)


plot(jags.out)

out=as.matrix(jags.out)
```

This model does a good job estimating the mean and variance parameters we could have gotten from a basic statistical summary. Now lets look at how well the model predicts the data. There are many ways to evaluate how well the modeled process captures the variability or signal in the data - and prediction is straightforward in the MCMC framework.  

JB: Make sure you draw from converged posterior distribution 
```{r}
NormMean.pred <- "
model {
  mu ~ dnorm(mu0,T) # prior on the mean 
  S ~ dgamma(a_obs,r_obs)

  for(i in 1:N){
    y[i] ~ dnorm(mu,S) # data model
    predy[i] ~ dnorm(mu,S) 
  }
}
"
```

We don't need to reset the inits...although if we were happy with the previous run then we might want to start this new run with the summary values for S and mu...

```{r}
j.model   <- jags.model (file = textConnection(NormMean.pred),
                             data = data,
                             inits = inits,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu","S"),
                                n.iter = 5000)


plot(jags.out)

```

Confirm convergence and define burn-in


```{r}
gelman.diag(jags.out)
GBR <- gelman.plot(jags.out)

```

```{r}
## determine the first iteration after convergence
burnin <- GBR$last.iter[tail(which(GBR$shrink[,,2] > 1.1),1)+1]## check for no burn-in case
if(length(burnin) == 0) burnin = 1
## remove burn-in
jags.burn <- window(jags.out,start=burnin)
## check diagnostics post burn-in
gelman.diag(jags.burn)
plot(jags.burn)
```

The burnin here was rapid. Now that we know the model converges quickly we can resample from the  posterior distribution of predy. 

```{r}

jags.out2   <- coda.samples (model = j.model,
                            variable.names = c("predy"),
                                n.iter = 1000)


out=as.matrix(jags.out2) ##technically we'd want to run more iterations and remove burnin period...

```

Now the matrix 'out' has a row for each MCMC iteration and a column for each of the N predicted y values. 

```{r}
predy.mean = rep(0,length(y))
for ( i in 1:length(y)){predy.mean[i] = mean(out[,i])}

par(mfrow=c(2,2))  #make space for 4 plots
hist(log(y))
hist(predy.mean)
plot(log(y),predy.mean)
cor.test(log(y),predy.mean)

```



One 'next step' would be to add a random effect for region. This makes sense if we expect different seasonality and road salt regimes might impact groups of nearby lakes. Below we'll add a random intercept effect - where regional intercepts are drawn from a normal distribution with a shared mean and variance. 



```{r}
NormMean.RE <- "
model {
  mu0 ~ dnorm(prior.mean,T) # prior on the mean 
  S ~ dgamma(a_obs,r_obs) #prior precision, data model

  for(i in 1:N){
    mu[i] <- mu0 + reg[region[i]] # process model
    y[i] ~ dnorm(mu[i],S) #data model
  }
 for(r in 1:maxR){  
    reg[r] ~ dnorm(0,tau_reg)
 }
tau_reg  ~ dgamma(0.01,0.01) ##prior precision,  random effect
}
"
```

Add new model with pH as covarite


```{r}
NormMean.Cov.RE <- "
model {
  beta0 ~ dnorm(prior.mean,T) # prior on the mean 
beta1~ dnorm(prior.ph,Tph)
  S ~ dgamma(a_obs,r_obs) #prior precision, data model

  for(i in 1:N){
    mu[i] <- beta0 + beta1 * ph.centered[i] + reg[region[i]] # process model
    y[i] ~ dnorm(mu[i],S) #data model
  }
 for(r in 1:maxR){  
    reg[r] ~ dnorm(0,tau_reg)
 }
tau_reg  ~ dgamma(0.01,0.01) ##prior precision,  random effect
}
"
```

Enter data - with regional indicator. Set initial conditions

```{r}
 
sigma=1.5
data = list(N=length(y),y = log(y), region = region, maxR=max(region), ## data
            prior.mean = 7.2, ## prior mean
            T = 1/sigma^2, ## prior precision
          a_obs = 0.01, r_obs= 0.01 )## prior mean precision 

nchain = 3
inits <- list()
for(i in 1:nchain){
  y.samp = sample(log(y),length(y),replace=TRUE)
  inits[[i]] <- list(S=1/var((y.samp)), mu0=mean(y.samp), tau_reg=1.2)
}

```


##Run the model
```{r}
j.model   <- jags.model (file = textConnection(NormMean.RE),
                             data = data,
                             inits = inits,
                             n.chains = 3)

```

```{r}
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu0","S","tau_reg"),
                                n.iter = 5000)



plot(jags.out)
```

Explore randome effects drawn for each region from  posterior distribution with mean 0 and precision tau_reg...
```{r}
jags.out2   <- coda.samples (model = j.model,
                            variable.names = c("reg"),
                                n.iter = 5000)

out<-as.matrix(jags.out2)

reg.mean = rep(0,max(region))
for (i in 1:max(region)){reg.mean[i] = mean(out[,i])}

```



