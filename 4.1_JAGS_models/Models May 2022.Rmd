---
title: "GLOEO space _ May2022"
author: "LaDeau"
date: "5/11/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, readxl, rjags, runjags, moments, coda)

#set a directory to use as a local file repository for plots if desire to write to file
#my_directory <- "C:/Users/ladeaus/Documents/GLEON/Brentrup" 
my_directory <- "~/Documents/Gloeo Bayesian Modeling/R Output/Bayes_model_calibration_output/"

```

# Read in Data Single Site
```{r}
###get calibration data - 1 site
#read in data matrix
gloeo <- read_csv("./00_Data_files/Bayesian_model_input_data/data_site_model_format.csv")
#gloeo is already on log scale, covars are already standardized


#model below requires data with k obs: y, covar1, covar2, week_avg1, week_avg2, 
#and constants mu.0, a_proc,r_proc, a_obs,r_obs, seasons_weeks, site_no
y=gloeo$y
covar1=gloeo$Temp
covar2=gloeo$GDD
week_avg1=gloeo$week_avg1
week_avg2=gloeo$week_avg2
mu.0=-10 ###-10 true value still on log scale but close to zero? -4.95 0+1/141.4 =0.007
a_obs=0.01; r_obs=0.01
a_proc=0.01; r_proc=0.01
season_weeks=gloeo$season_weeks
year_no = gloeo$year_no
#year_no = c(1:8)
```

# Read in Data Multi Site ###
```{r}
###get calibration data - 1 site
#read in data matrix
gloeo <- read_csv("./00_Data_files/Bayesian_model_input_data/data_4site_model_format.csv")
#gloeo is already on log scale, covars are already standardized


#model below requires data with k obs: y, covar1, covar2, week_avg1, week_avg2, 
#and constants mu.0, a_proc,r_proc, a_obs,r_obs, seasons_weeks, site_no
y=as.matrix(gloeo[,3:5])
covar1=as.matrix(gloeo[,7:9])
covar2=as.matrix(gloeo[,11:13])
week_avg1=gloeo$week_avg1
week_avg2=gloeo$week_avg2
mu.0=-10 ###-10 true value still on log scale but close to zero? -4.95 0+1/141.4 =0.007
a_obs=0.01; r_obs=0.01
a_proc=0.01; r_proc=0.01 #0.001
season_weeks=c(1:160)
year_no = as.numeric(as.factor(gloeo$year))

```

# Read in gloeo data pre-2009 for observation error prior
```{r}
pre_gloeo <- read_csv("00_Data_files/EDI_data_clones/weekly_surface_gloeo_4sites_2005_2016_v10April2020.csv",
      col_types = list(
      date = col_date(format = ""),
      site = col_character(),
      year = col_double(),
      dayofyr = col_double(),
      n_sample = col_double(),
      coloniesperL = col_double(),
      filbundperL = col_double(),
      totalperL = col_double()
    )
  )

```

# Data wrangling for pre-gloeo data
```{r}
# Filter for pre-2009 data all sites
pre_gloeo2 <- pre_gloeo %>% 
  filter(year %in% 2007:2008)
 
# Add in missing observations for Newbury 2007
date <- c("2007-06-21", "2007-06-28", "2008-06-05")
site <- c("Newbury", "Newbury", "NorthSunapeeHarbor")

odd_obs <- bind_cols(date = date, site = site) %>% 
  mutate(date = as.Date(date)) %>%
  mutate(year = year(date), dayofyr = yday(date)) %>%
  mutate(n_sample = 0, coloniesperL = NA, filbundperL = NA, totalperL = NA)

pre_gloeo3 <- bind_rows(pre_gloeo2, odd_obs) %>% 
  arrange(date, site) %>% 
  mutate(ln_totalperL = log(totalperL + (1/141.4))) %>%  # add ln of gloeo, Total per L is Volume of 2, ~1 m net tows = 141.4 L
  select(date,year,dayofyr,site, totalperL, ln_totalperL) # rearrange columns to have date items together

ggplot(pre_gloeo3, aes(y = ln_totalperL, x = date))+
  geom_point()+
  geom_line()+
  facet_wrap(~site)

# Format for models

pre_gloeo4 <- pre_gloeo3 %>% 
  pivot_wider(c(date, ln_totalperL,site), values_from = ln_totalperL, names_from = site) %>% 
  mutate(season_week = 1:31)

colnames(pre_gloeo4) <- c("date","hc_gloeo_ln", "nb_gloeo_ln", "nsh_gloeo_ln", "sotf_gloeo_ln", "season_week")

y = as.matrix(pre_gloeo4[,2:4])
year_no = as.numeric(as.factor(2007:2008))
season_weeks = pre_gloeo4$season_week # full season weeks instead of 1:20
site_no = c(1:4)

# single site

hc_pre_gloeo <- pre_gloeo %>% 
  filter(site == "HerrickCoveSouth" & year < 2009) %>% 
  arrange(date) %>% 
  mutate(ln_totalperL = log(totalperL + (1/141.4))) %>%   # add ln of gloeo, Total per L is Volume of 2, ~1 m net tows = 141.4 L
  mutate(season_week = 1:67)

hist(hc_pre_gloeo$ln_totalperL)

ggplot(hc_pre_gloeo, aes(x = ln_totalperL))+
  geom_histogram(bins = 7, color = "white")+
  facet_wrap(~year)
```

# Histogram for pre gloeo data
```{r}
ggplot(pre_gloeo3, aes(x = ln_totalperL))+
  geom_histogram(bins = 30)+
  facet_wrap(year ~ site)

# Filter for each year
pre_gloeo_2007 <- pre_gloeo3 %>% 
  filter(year == 2007) #%>% 
  filter(site == "HerrickCoveSouth")

pre_gloeo_2008 <- pre_gloeo3 %>% 
  filter(year == 2008) #%>% 
  filter(site == "HerrickCoveSouth")

hist(pre_gloeo_2007$ln_totalperL, ylim = c(0,15))
hist(pre_gloeo_2008$ln_totalperL)

hist(pre_gloeo_2007$totalperL)
hist(pre_gloeo_2008$totalperL)

pre_gloeo_2007 %>% 
  group_by(site) %>% 
  summarize(mean = mean(totalperL, na.rm = T))

pre_gloeo_2008 %>% 
  group_by(site) %>% 
  summarize(mean = mean(totalperL, na.rm = T))

summary(pre_gloeo_2007)
summary(pre_gloeo_2008)

```

## Random Walk - Pre-2009 Herrick Cove gloeo data for observation error 1 site model

```{r}

y = pre_gloeo4$hc_gloeo_ln #2007 & 2008
y = hc_pre_gloeo$ln_totalperL # all years
#year_no = as.numeric(as.factor(2007:2008))
season_weeks = pre_gloeo4$season_week # 1:31
season_weeks = hc_pre_gloeo$season_week # 1:67 uneven timesteps 3-8 days

mu.0=-10 ###-10 true value still on log scale but close to zero? -4.95 0+1/141.4 =0.007
a_obs = 0.001; r_obs = 0.001 # uninformed priors 0.001
#a_obs = 0.152; r_obs = 0.0011 # informed priors 13.088, 25.513
a_proc=0.001; r_proc=0.001 #0.001

##Random Walk run - converges fine
data.rw <- list(y=y,season_weeks=season_weeks,x_ic=-5,tau_ic=100,a_obs=a_obs,r_obs=r_obs,a_proc=a_proc,r_proc=r_proc)

RandomWalk = "
model{
  
  #### Data Model
  for(t in 1:max(season_weeks)){
    y[t] ~ dnorm(x[t],tau_obs)
  }
  
  #### Process Model
  for(t in 2:max(season_weeks)){
    x[t]~dnorm(x[t-1],tau_proc)
  }
  
  #### Priors
  x[1] ~ dnorm(x_ic,tau_ic)
  tau_obs ~ dgamma(a_obs,r_obs)
  tau_proc ~ dgamma(a_proc,r_proc)
}
"

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(y.samp),na.rm=T),tau_obs=5/var(y.samp,na.rm=T))}

init <- list(list(tau_proc=0.001, tau_obs = 0.1), list(tau_proc=0.1, tau_obs = 1), list(tau_proc=1, tau_obs = 5))

  
  
j.model_RW   <- jags.model (file = textConnection(RandomWalk),
                             data = data.rw,
                             inits = init,
                             n.chains = 3)  



jags.out_RW   <- coda.samples (model = j.model_RW,
                            variable.names = c("tau_proc","tau_obs"), # watch x
                            n.iter = 50000)
plot(jags.out_RW)

summary(jags.out_RW)

burnin = 10000                              ## determine convergence
jags.burn_RW <- window(jags.out_RW,start=burnin) 

summary(jags.burn_RW)
plot(jags.burn_RW)

gelman.diag(jags.out_RW)

GBR <- gelman.plot(jags.out_RW) #The point up to where the GBR drops below 1.05 is termed the "burn in" period

# Calculate DIC
DIC.RW <- dic.samples(j.model_RW, n.iter=5000)
DIC.RW
```


## Random Walk - Pre 2009 gloeo data for observation error 4 sites

```{r}
y = as.matrix(pre_gloeo4[,2:5]) # include Fichter?

#year_no = as.numeric(as.factor(2007:2008))
season_weeks = pre_gloeo4$season_week
site_no = c(1:4)

a_obs = 0.1; r_obs = 0.1 # uninformed priors 0.001
#a_obs = 0.152; r_obs = 0.0011 # informed priors 13.088, 25.513
a_proc=0.1; r_proc=0.1 #0.001


##Random Walk run - converges fine
data.rw <- list(y=y,n=length(season_weeks),site_no = site_no, x_ic=-5, tau_ic=100, a_obs=a_obs, r_obs=r_obs, a_proc=a_proc,  r_proc=r_proc)

RandomWalk = "
model{
  
for(j in 1:max(site_no)) {

  #### Data Model
  for(t in 1:n){
    y[t,j] ~ dnorm(x[t,j],tau_obs)
  }
  
  #### Process Model
  for(t in 2:n){
    x[t,j]~dnorm(x[t-1,j],tau_proc)
  }
  
 #### Initial Conditions
  x[1,j] ~ dnorm(x_ic,tau_ic)

}
 #### Priors
  tau_obs ~ dgamma(a_obs,r_obs)
  tau_proc ~ dgamma(a_proc,r_proc)
}
"

# nchain = 3
# init <- list()
# for(i in 1:nchain){
#   y.samp = sample(y,length(y),replace=TRUE)
#   init[[i]] <- list(tau_obs=5/var(y.samp,na.rm=T))}

init <- list(list(tau_proc=0.001, tau_obs = 0.1), list(tau_proc=0.1, tau_obs = 1), list(tau_proc=1, tau_obs = 5))

  
  
j.model_RW_multi   <- jags.model (file = textConnection(RandomWalk),
                             data = data.rw,
                             inits = init,
                             n.chains = 3)  



jags.out_RW_multi   <- coda.samples (model = j.model_RW_multi,
                            variable.names = c("tau_proc","tau_obs"),
                            n.iter = 50000)
plot(jags.out_RW_multi)

summary(jags.out_RW_multi)

gelman.diag(jags.out_RW_multi)

GBR <- gelman.plot(jags.out_RW_multi) #The point up to where the GBR drops below 1.05 is termed the "burn in" period


burnin = 30000                              ## determine convergence
jags.burn_RW_multi <- window(jags.out_RW_multi,start=burnin) 

summary(jags.burn_RW_multi) 
plot(jags.burn_RW_multi)

gelman.diag(jags.burn_RW_multi)

# Calculate DIC
DIC.RW <- dic.samples(j.model_RW_multi, n.iter=5000)
DIC.RW
```

28 June 2022 - used 2007 & 2008 data from all 4 sites, a/r=0.1, burnin 30000
mean = 1.59, SD = 2.1349
r = 0.349
a = 0.555

summary(jags.burn_RW_multi)

Iterations = 30000:50000
Thinning interval = 1 
Number of chains = 3 
Sample size per chain = 20001 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean     SD Naive SE Time-series SE
tau_obs  1.590 2.1349 0.008716         0.1070
tau_proc 0.573 0.1872 0.000764         0.0038

2. Quantiles for each variable:

           2.5%    25%    50%   75% 97.5%
tau_obs  0.5946 0.8506 1.0801 1.481 6.785
tau_proc 0.2794 0.4379 0.5498 0.682 1.006

gelman.diag(jags.burn_RW_multi)
Potential scale reduction factors:

         Point est. Upper C.I.
tau_obs           1          1
tau_proc          1          1

Multivariate psrf

1

## Random Walk - Fichter 2009-2014 gloeo data for observation error as matrix

```{r}

#set calibration years and weeks of season - do not edit
years <- c(2009:2014)
year_no = as.numeric(as.factor(years))
season_weeks = c(1:20)

#read in Gloeo data
y <- as.matrix(read_csv("./00_Data_files/Bayesian_model_input_data/Gechinulata_Site2.csv"))
#remove 2015-2016 data
y <- y[-c(7:8),]

#model below requires data with k obs: y, covar1, covar2, week_avg1, week_avg2, 
#and constants mu.0, a_proc,r_proc, a_obs,r_obs, seasons_weeks, site_no
mu.0=-10 ###-10 true value still on log scale but close to zero? -4.95 0+1/141.4 =0.007
a_obs = 0.001; r_obs = 0.001 # uninformed priors
a_proc=0.001; r_proc=0.001 #0.01

data.rw <- list(y=y,n=season_weeks,year_no = year_no, x_ic=-5,tau_ic=100,a_obs=a_obs,r_obs=r_obs,a_proc=a_proc,r_proc=r_proc)

#x_ic=-5,tau_ic=100,

RandomWalk = "
model{

for(k in 1:max(year_no)){
  
  #### Data Model
  for(t in 1:max(n)){
    y[k,t] ~ dnorm(x[k,t],tau_obs)
  }
  
  #### Process Model
  for(t in 2:max(n)){
    x[k,t]~dnorm(x[k,t-1],tau_proc)
  }
  
  #### Priors
  x[k,1] ~ dnorm(x_ic,tau_ic)

  }

  tau_obs ~ dgamma(a_obs,r_obs)
  tau_proc ~ dgamma(a_proc,r_proc)
}
"

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(y.samp),na.rm=T),tau_obs=5/var(y.samp,na.rm=T))}

init <- list(list(tau_proc=0.001, tau_obs = 0.1), list(tau_proc=0.1, tau_obs = 1), list(tau_proc=1, tau_obs = 5))

  
  
j.model_RW   <- jags.model (file = textConnection(RandomWalk),
                             data = data.rw,
                             inits = init,
                             n.chains = 3)  



jags.out_RW   <- coda.samples (model = j.model_RW,
                            variable.names = c("tau_proc","tau_obs"), # watch x
                            n.iter = 50000)
plot(jags.out_RW)

summary(jags.out_RW)

burnin = 10000                              ## determine convergence
jags.burn_RW <- window(jags.out_RW,start=burnin) 

summary(jags.burn_RW)
plot(jags.burn_RW)

jags.out <- run.jags(model = RandomWalk,
                     data = data.rw,
                     adapt =  5000,
                     burnin =  10000,
                     sample = 50000, #50000
                     n.chains = 3,
                     inits=init,
                     monitor = c("tau_proc","tau_obs"))

summary(jags.out)
```

## Random Walk run - converges fine 1 site

```{r}
##Random Walk run - converges fine
data.rw <- list(y=y,n=length(season_weeks),x_ic=log(1000),tau_ic=100,a_obs=1,r_obs=1,a_add=1,r_add=1)

RandomWalk = "
model{
  
  #### Data Model
  for(t in 1:n){
    y[t] ~ dnorm(x[t],tau_obs)
  }
  
  #### Process Model
  for(t in 2:n){
    x[t]~dnorm(x[t-1],tau_add)
  }
  
  #### Priors
  x[1] ~ dnorm(x_ic,tau_ic)
  tau_obs ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_add,r_add)
}
"

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(y.samp),na.rm=T),tau_obs=5/var(y.samp,na.rm=T))}
  
  
j.model_RW   <- jags.model (file = textConnection(RandomWalk),
                             data = data.rw,
                             inits = init,
                             n.chains = 3)  



jags.out_RW   <- coda.samples (model = j.model_RW,
                            variable.names = c("tau_add","tau_obs"),
                            n.iter = 1000)
plot(jags.out_RW)
```


## Random Walk - 3 sites

```{r}
##Random Walk run - converges fine
data.rw <- list(y=y,n=length(season_weeks),site_no = 1:3,x_ic=-5,tau_ic=100,a_obs=0.001,r_obs=0.001,a_add=0.001,r_add=0.001)



RandomWalk = "
model{
  
for(j in 1:max(site_no)) {

  #### Data Model
  for(t in 1:n){
    y[t,j] ~ dnorm(x[t,j],tau_obs)
  }
  
  #### Process Model
  for(t in 2:n){
    x[t,j]~dnorm(x[t-1,j],tau_add)
  }
  
#Initial conditions
  x[1,j] ~ dnorm(x_ic,tau_ic)

  }

  #### Priors
  tau_obs ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_add,r_add)
}
"

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(y.samp),na.rm=T),tau_obs=5/var(y.samp,na.rm=T))}
  
  
j.model_RW   <- jags.model (file = textConnection(RandomWalk),
                             data = data.rw,
                             inits = init,
                             n.chains = 3)  



jags.out_RW   <- coda.samples (model = j.model_RW,
                            variable.names = c("tau_add","tau_obs"),
                            n.iter = 50000)
plot(jags.out_RW)

summary(jags.out_RW)
```


## Dynamic linear
```{r}

data <- list(y=y,WK=length(season_weeks), covar1=covar1, covar2=covar2, week_avg1=week_avg1, week_avg2=week_avg2, a_obs=a_obs, r_obs=r_obs, a_proc=a_proc,r_proc=r_proc,x_ic=-6,tau_ic=0.1)

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(y.samp),na.rm=T),tau_obs=5/var(y.samp,na.rm=T))}
  

mod1 = "
model{
  
  #### Data Model
  for(t in 1:WK){
    y[t] ~ dnorm(x[t],tau_obs)
        covar1[t] ~ dnorm(week_avg1[t],0.1)
        covar2[t] ~ dnorm(week_avg2[t],0.1)

  }
  
  #### Process Model
  for(t in 2:WK){
    x[t]~dnorm(lam[t],tau_add)
    lam[t]=beta1 + beta2*x[t-1] + beta3*covar1[t] + beta4 * covar2[t] + beta5 * covar2[t]^2 # lam or x on beta2? x trace plots look better
}
  
  #### Priors
  x[1] ~ dnorm(x_ic,tau_ic)
 # lam[1] ~ dnorm(x_ic,tau_ic) - not needed but DIC lower?

  beta1~dnorm(0,0.01)
  beta2~dnorm(0,0.01)
  beta3~dnorm(0,0.01)
  beta4~dnorm(0,0.01)
  beta5~dnorm(0,0.01)

  tau_obs ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_proc,r_proc)
}
"

j.model_DL   <- jags.model (file = textConnection(mod1),
                             data = data,
                             inits = init,
                             n.chains = 3)  



jags.out_DL   <- coda.samples (model = j.model_DL,
                            variable.names = c("tau_add","tau_obs","beta1","beta2","beta3","beta4","beta5"),
                            n.iter = 10000)
plot(jags.out_DL)

summary(jags.out_DL)

DL_var.mat <- as.matrix(jags.out_DL)
pairs(DL_var.mat)
cor(DL_var.mat)

# Calculate DIC
DIC.DL <- dic.samples(j.model_DL, n.iter=5000)
DIC.DL

# no lam initial conditions, just x
# > DIC.DL
# Mean deviance:  1384 
# penalty 898.3 
# Penalized deviance: 2282 

# with lam initial conditions
# > DIC.DL
# Mean deviance:  1370 
# penalty 680.7 
# Penalized deviance: 2051 

```

## Dynamic linear with Random Year Effect
```{r}

data <- list(y=y,
             WK=length(season_weeks), 
             year_no = year_no,
             totYr = length(unique(year_no)),
             covar1=covar1, 
             covar2=covar2, 
             week_avg1=week_avg1, 
             week_avg2=week_avg2, 
             a_obs=a_obs, r_obs=r_obs,
             a_proc=a_proc,r_proc=r_proc,
             x_ic=-6,tau_ic=0.1)


mod1 = "
model{

  #### Data Model
  for(t in 1:WK){
    y[t] ~ dnorm(x[t],tau_obs)
        covar1[t] ~ dnorm(week_avg1[t],0.1)
        covar2[t] ~ dnorm(week_avg2[t],0.1)

  }
  
  #### Process Model 
  for(t in 2:WK){
    x[t]~dnorm(lam[t],tau_add)
    lam[t]=beta1 + beta2*x[t-1] + beta3*covar1[t] + beta4 * covar2[t] + beta5 * covar2[t]^2 + yr[year_no[t]]
  
}
  
  #### Priors

for(k in 1:totYr){
  yr[k] ~ dnorm(0, tau_yr) # k loop on yr
  }

  x[1] ~ dnorm(x_ic,tau_ic)
  lam[1] ~ dnorm(x_ic,tau_ic)

  beta1~dnorm(0,0.01)
  beta2~dnorm(0,0.01)
  beta3~dnorm(0,0.01)
  beta4~dnorm(0,0.01)
  beta5~dnorm(0,0.01)

  tau_obs ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_proc,r_proc)
  tau_yr ~ dgamma(0.01,0.01)
  
}
"

j.model_DL_RY   <- jags.model (file = textConnection(mod1),
                             data = data,
                             inits = init,
                             n.chains = 3)  



jags.out_DL_RY   <- coda.samples (model = j.model_DL_RY,
                            variable.names = c("beta1","beta2","beta3","beta4","beta5", "tau_add","tau_obs", "tau_yr", "yr"),
                            n.iter = 10000)
plot(jags.out_DL_RY)

summary(jags.out_DL_RY)

var.mat <- as.matrix(jags.out_DL_RY)
pairs(var.mat)
cor(var.mat)

# Calculate DIC
DIC.DL_RY <- dic.samples(j.model_DL_RY, n.iter=5000)
DIC.DL_RY
```


## Dynamic linear with multiple sites


```{r}

data <- list(y=y,
             WK=length(season_weeks), 
             site_no = 1:3,
             covar1=covar1, 
             covar2=covar2, 
             week_avg1=week_avg1, 
             week_avg2=week_avg2, 
             a_obs=a_obs, r_obs=r_obs, 
             a_proc=a_proc,r_proc=r_proc,
             x_ic=-6,tau_ic=0.1)


mod1 = "
model{
  
  #### Data Model
for(j in 1:max(site_no)) {

  for(t in 1:WK){
    y[t,j] ~ dnorm(x[t,j],tau_obs)
        covar1[t,j] ~ dnorm(week_avg1[t],tau_C1_proc) #make a matrix
        covar2[t,j] ~ dnorm(week_avg2[t],tau_C2_proc)

  }
  
  #### Process Model
  for(t in 2:WK){
    x[t,j]~dnorm(lam[t,j],tau_add)
    lam[t,j]=beta1 + beta2*x[t-1,j] + beta3*covar1[t,j] + beta4*covar2[t,j] + beta5*covar2[t,j]^2
}
  

  #### Initial Conditions
  x[1,j] ~ dnorm(x_ic,tau_ic)
  lam[1,j] ~ dnorm(x_ic,tau_ic)

}



#### Priors
  beta1~dnorm(0,0.001)
  beta2~dnorm(0,0.001)
  beta3~dnorm(0,0.001)
  beta4~dnorm(0,0.001)
  beta5~dnorm(0,0.001)

  tau_C1_proc ~ dgamma(0.01,0.01)
  tau_C2_proc ~ dgamma(0.01,0.01)
  tau_obs ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_proc,r_proc)
  
}
"

j.model_DL_3sites   <- jags.model (file = textConnection(mod1),
                             data = data,
                             inits = init,
                             n.chains = 3)  



jags.out_DL_3sites   <- coda.samples (model = j.model_DL_3sites,
                            variable.names = c("tau_add","tau_obs","beta1","beta2","beta3","beta4","beta5"),
                            n.iter = 50000)
plot(jags.out_DL_3sites)

summary(jags.out_DL_3sites)
gelman.diag(jags.out_DL_3sites)

GBR <- gelman.plot(jags.out_DL_3sites) #The point up to where the GBR drops below 1.05 is termed the "burn in" period


burnin = 20000                              ## determine convergence
jags.burn_3 <- window(jags.out_DL_3sites,start=burnin)  ## remove burn-in
plot(jags.burn_3)

gelman.diag(jags.burn_3)
summary(jags.burn_3)

DL_var.mat <- as.matrix(jags.out_DL_3sites)
pairs(DL_var.mat)
cor(DL_var.mat)

# Calculate DIC
DIC.multi <- dic.samples(j.model_DL_3sites, n.iter=5000)
DIC.multi
  
```


## Dynamic linear with Multiple sites + Random Year Effect
```{r}

data <- list(y=y,
            WK=length(season_weeks), 
            site_no = length(1:3),
             year_no = year_no, # same length as season weeks
             totYr = length(unique(year_no)), # vector length of total number of years 8
             covar1=covar1, 
             covar2=covar2, 
             week_avg1=week_avg1, 
             week_avg2=week_avg2, 
             a_obs=a_obs, r_obs=r_obs,
             a_proc=a_proc,r_proc=r_proc,
             x_ic=-5,tau_ic=100)

# init <- list()
# for(i in 1:nchain){
#   y.samp = sample(y,length(y),replace=TRUE)
#   init[[i]] <- list(tau_add=1/var(diff(y.samp),na.rm=T),tau_obs=5/var(y.samp,na.rm=T))}

init <- list(list(tau_add=0.001, tau_obs = 0.1,tau_C1_proc = 0.01,tau_C2_proc = 0.01, 
                  beta1=-2, beta2=-0.5, beta3=0, beta4=0, beta5=0),
    list(tau_add=0.1,tau_obs = 1,tau_C1_proc = 0.1,tau_C2_proc = 0.1, 
         beta1=-1.5, beta2=0, beta3=0.5, beta4=0.5, beta5=0.5),
    list(tau_add=1, tau_obs = 5,tau_C1_proc = 1,tau_C2_proc = 1, 
         beta1=-1,beta2=0.5, beta3=1, beta4=1, beta5=1))

# init <- list()
# for(i in 1:nchain){
#   y.samp = sample(y,length(y),replace=TRUE)
#   init[[i]] <- list(tau_add=1/var(diff(y.samp),na.rm=T),tau_obs=5/var(y.samp,na.rm=T))}

mod1 = "
model{
  
  #### Data Model
for(j in 1:site_no) {

  for(t in 1:WK){
    y[t,j] ~ dnorm(x[t,j],tau_obs)
        covar1[t,j] ~ dnorm(week_avg1[t],tau_C1_proc) #make a matrix
        covar2[t,j] ~ dnorm(week_avg2[t],tau_C2_proc)

  }
  
  #### Process Model
  for(t in 2:WK){
    x[t,j]~dnorm(lam[t,j],tau_add)
    lam[t,j]=beta1 + beta2*x[t-1,j] + beta3*covar1[t,j] + beta4*covar2[t,j] + beta5*covar2[t,j]^2 + yr[year_no[t]] # changed to x instead of lam
    #lam[t,j]=beta[1] + beta[2]*x[t-1,j] + beta[3]*covar1[t,j] + beta[4]*covar2[t,j] + beta[5]*covar2[t,j]^2 + yr[year_no[t]]

}
  

  #### Initial Conditions
  x[1,j] ~ dnorm(x_ic,tau_ic)
  lam[1,j] ~ dnorm(x_ic,tau_ic)

}



for(k in 1:totYr){
  yr[k] ~ dnorm(beta1, tau_yr) # k loop on yr, centered on beta1 instead of 0 helps!
}


#### Priors

# for(i in 1:5) {
#   beta[i] ~ dnorm(0,0.001) #worse, need to set init values somehow
# }


  beta1~dnorm(0,0.001)
  beta2~dnorm(0,0.001)
  beta3~dnorm(0,0.001)
  beta4~dnorm(0,0.001)
  beta5~dnorm(0,0.001)

  tau_C1_proc ~ dgamma(0.01,0.01)
  tau_C2_proc ~ dgamma(0.01,0.01)
  tau_obs ~ dgamma(a_obs,r_obs)
  tau_add ~ dgamma(a_proc,r_proc)
  tau_yr ~ dgamma(0.01,0.01) #keep prior in same space
  
}
"

j.model_DL_RY_3sites   <- jags.model (file = textConnection(mod1),
                             data = data,
                             inits = init,
                             n.chains = 3)  



jags.out_DL_RY_3sites  <- coda.samples (model = j.model_DL_RY_3sites,
                            variable.names = c("beta1","beta2","beta3","beta4","beta5", "tau_add","tau_obs", "tau_yr", "yr"), 
                            n.iter = 50000) #50000

#"beta[1]","beta[2]","beta[3]","beta[4]","beta[5]",
#"beta1","beta2","beta3","beta4","beta5", 
plot(jags.out_DL_RY_3sites)

summary(jags.out_DL_RY_3sites)
gelman.diag(jags.out_DL_RY_3sites)

GBR <- gelman.plot(jags.out_DL_RY_3sites) #The point up to where the GBR drops below 1.05 is termed the "burn in" period

burnin = 20000                              ## determine convergence
jags.burn <- window(jags.out_DL_RY_3sites,start=burnin)  ## remove burn-in
plot(jags.burn)

gelman.diag(jags.burn)
summary(jags.burn)
var.mat <- as.matrix(jags.out_DL_RY_3sites)
cor(var.mat)
pairs(var.mat)


# Calculate DIC
DIC.multi_RY <- dic.samples(j.model_DL_RY_3sites, n.iter=5000)
DIC.multi_RY
```

# GLM to find initial values for betas
```{r}
multi_RY_fit <- glm(gloeo$hc_gloeo_ln ~ gloeo$HCS.tempC_min_stand + gloeo$GDD_hc + (gloeo$GDD_hc^2))
summary(multi_RY_fit)

multi_RY_fit2 <- lm(gloeo$hc_gloeo_ln ~ gloeo$HCS.tempC_min_stand + gloeo$GDD_hc + (gloeo$GDD_hc^2))
summary(multi_RY_fit2)

multi_RY_fit2 <- lm(gloeo$nb_gloeo_ln ~ gloeo$NB.tempC_min_stand + gloeo$GDD_nb + (gloeo$GDD_nb^2))
summary(multi_RY_fit2)

multi_RY_fit2 <- lm(gloeo$nsh_gloeo_ln ~ gloeo$NSH.tempC_min_stand + gloeo$GDD_nsh + (gloeo$GDD_nsh^2))
summary(multi_RY_fit2)
```


## Ho Model - single site (HC) 1 covariate min water temp

Add in linear model on standard deviation, allows for variance to be non-constant
 s[i] <- alpha1 + alpha2*x[i]  ## linear model on standard deviation of x
 
  S[i] <- 1/s[i]^2  ## calculate precision from SD
 
```{r}

# Use single Herrick Cove data

a_obs = 0.01; r_obs = 0.01

data <- list(y=y,
             WK=length(season_weeks), 
             covar1=covar1, 
            # covar2=covar2, 
             week_avg1=week_avg1, 
            # week_avg2=week_avg2, 
             a_obs=a_obs, r_obs=r_obs, 
            # a_proc=a_proc,r_proc=r_proc,
             x_ic=-5,tau_ic=100) #0.1

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_obs=5/var(y.samp,na.rm=T))}


init <- list(list(tau_obs = 0.1,
          beta1=-1, beta2=0.2, beta3=0.2, alpha1 = 0.1, alpha2 = 0.005),
    list(tau_obs = 1,
         beta1=-0.5, beta2=0.7, beta3=0.5, alpha1 = 1, alpha2 = 0.03),
    list(tau_obs = 5,
         beta1=0,beta2=1, beta3=1, alpha1 = 2, alpha2 = 0.5))

# alpha2 - hardest to converge

mod1 = "
model{
  
  #### Data Model
  for(t in 1:WK){
    y[t] ~ dnorm(x[t],tau_obs)
        covar1[t] ~ dnorm(week_avg1[t],0.1)
        #covar2[t] ~ dnorm(week_avg2[t],0.1)

  }
  
  #### Process Model
  for(t in 2:WK){
    x[t]~dnorm(lam[t],S_add[t]) # replace tau_add with linear model
    lam[t]=beta1 + beta2*x[t-1] + beta3*covar1[t]
    #S_add[t] = alpha1 + alpha2*lam[t] 
    S_add[t] = 1/(alpha1 + alpha2*x[t-1])^2 # written as precision 1/sd squared, lam or x??,pow(alpha1 + alpha2*lam[t], 2)
  }


  #### Priors # add prior for alphas
  x[1] ~ dnorm(x_ic,tau_ic)
  lam[1] ~ dnorm(x_ic,tau_ic)


  beta1~dnorm(0,0.001)
  beta2~dnorm(0,0.001)
  beta3~dnorm(0,0.001)
  #beta4~dnorm(0,0.001)
  #beta5~dnorm(0,0.001)

  alpha1~dlnorm(0,0.01) #log normal variance can't be negative, was @ 0.001 not converging well
  alpha2~dlnorm(0,0.01) #log normal variance can't be negative

  tau_obs ~ dgamma(a_obs,r_obs)
  #tau_C1_proc ~ dgamma(0.01,0.01)
  #tau_C2_proc ~ dgamma(0.01,0.01)

  #tau_add ~ dgamma(a_proc,r_proc)

}
"

j.model_Hetero   <- jags.model (file = textConnection(mod1),
                             data = data,
                             inits = init,
                             n.chains = 3)  



jags.out_Hetro   <- coda.samples (model = j.model_Hetero,
                            variable.names = c("tau_obs","beta1","beta2","beta3", "alpha1", "alpha2"), #"tau_add",
                            n.iter = 50000)
plot(jags.out_Hetro)

summary(jags.out_Hetro)

gelman.diag(jags.out_Hetro)

GBR <- gelman.plot(jags.out_Hetro) #The point up to where the GBR drops below 1.05 is termed the "burn in" period


burnin = 10000                              ## determine convergence
jags.burn_Hetero <- window(jags.out_Hetro,start=burnin)  ## remove burn-in
plot(jags.burn_Hetero)

summary(jags.burn_Hetero)
gelman.diag(jags.burn_Hetero)


DL_var.mat <- as.matrix(jags.out_Hetro)
pairs(DL_var.mat)
cor(DL_var.mat)

  

# Calculate DIC
DIC.hetero <- dic.samples(j.model_Hetero, n.iter=5000)
DIC.hetero #lower than homo
  
```

## Ho Model - single site (HC) 2 covariates min water temp & GDD

Add in linear model on standard deviation, allows for variance to be non-constant
 s[i] <- alpha1 + alpha2*x[i]  ## linear model on standard deviation of x
 
  S[i] <- 1/s[i]^2  ## calculate precision from SD
 
```{r}

# Use single Herrick Cove data

a_obs = 0.01; r_obs = 0.01

data <- list(y=y,
             WK=length(season_weeks), 
             covar1=covar1, 
            covar2=covar2, 
             week_avg1=week_avg1, 
            week_avg2=week_avg2, 
             a_obs=a_obs, r_obs=r_obs, 
            # a_proc=a_proc,r_proc=r_proc,
             x_ic=-5,tau_ic=100) #0.1

nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_obs=5/var(y.samp,na.rm=T))} # no inits works better - why?


init <- list(list(tau_obs = 0.1,
          beta1=-1, beta2=0.2, beta3=0, beta4 =-0.5, beta5 =-0.5,alpha1 = 0.1, alpha2 = 0.005),
    list(tau_obs = 1,
         beta1=-0.5, beta2=0.7, beta3=0.5, beta4=0, beta5=0, alpha1 = 1, alpha2 = 0.03),
    list(tau_obs = 5,
         beta1=0,beta2=1, beta3=1, beta4=0.5, beta5 =0.5, alpha1 = 2, alpha2 = 0.5))

# alpha2 - hardest to converge

mod1 = "
model{
  
  #### Data Model
  for(t in 1:WK){
    y[t] ~ dnorm(x[t],tau_obs)
        covar1[t] ~ dnorm(week_avg1[t],0.01)
        covar2[t] ~ dnorm(week_avg2[t],0.01)

  }
  
  #### Process Model
  for(t in 2:WK){
    x[t]~dnorm(lam[t],S_add[t]) # replace tau_add with linear model
    lam[t]=beta1 + beta2*x[t-1] + beta3*covar1[t] + beta4*covar2[t] + beta5*covar2[t]^2
    S_add[t] = 1/(alpha1 + alpha2*x[t-1])^2 # written as precision 1/sd squared, lam or x??,pow(alpha1 + alpha2*lam[t], 2)
  }


  #### Priors # add prior for alphas
  x[1] ~ dnorm(x_ic,tau_ic)
  lam[1] ~ dnorm(x_ic,tau_ic)


  beta1~dnorm(0,0.001)
  beta2~dnorm(0,0.001)
  beta3~dnorm(0,0.001)
  beta4~dnorm(0,0.001)
  beta5~dnorm(0,0.001)

  alpha1~dlnorm(0,0.01) #log normal variance can't be negative, was @ 0.001 not converging well
  alpha2~dlnorm(0,0.01) #log normal variance can't be negative

  tau_obs ~ dgamma(a_obs,r_obs)
  #tau_C1_proc ~ dgamma(0.01,0.01)
  #tau_C2_proc ~ dgamma(0.01,0.01)

  #tau_add ~ dgamma(a_proc,r_proc)

}
"

j.model_Hetero   <- jags.model (file = textConnection(mod1),
                             data = data,
                             inits = init,
                             n.chains = 3)  



jags.out_Hetro   <- coda.samples (model = j.model_Hetero,
                            variable.names = c("tau_obs","beta1","beta2","beta3","beta4","beta5", "alpha1", "alpha2"), #"tau_add",
                            n.iter = 50000)
plot(jags.out_Hetro)

summary(jags.out_Hetro)

gelman.diag(jags.out_Hetro)

GBR <- gelman.plot(jags.out_Hetro) #The point up to where the GBR drops below 1.05 is termed the "burn in" period


burnin = 20000                              ## determine convergence
jags.burn_Hetero <- window(jags.out_Hetro,start=burnin)  ## remove burn-in
plot(jags.burn_Hetero)

summary(jags.burn_Hetero)
gelman.diag(jags.burn_Hetero)

DL_var.mat <- as.matrix(jags.out_Hetro)
pairs(DL_var.mat)
cor(DL_var.mat)

# Calculate DIC
DIC.hetero <- dic.samples(j.model_Hetero, n.iter=5000)
DIC.hetero #lower than homo
  
```

