---
title: "Dietze Ch. 11 Uncertainty Analysis-Copy"
author: "J. Brentrup"
date: "6/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries
```{r}
library(ecoforecastR)
pacman::p_load(tidyverse, readxl, rjags, runjags, moments, coda)

```

## Load data for full time series and covariates (uses Fichter but can update)
```{r}
gloeo <- read_csv("./00_Data_files/Bayesian_model_input_data/data_site_model_format.csv")

# fill in missing holes 
missing <- which(is.na(gloeo$Temp))
x <- median(gloeo$Temp, na.rm = T)
z <- median(gloeo$GDD, na.rm = T)

gloeo$Temp2 <- gloeo$Temp %>% 
  replace_na(x)

gloeo$GDD2 <- gloeo$GDD %>% 
  replace_na(z)
  
```


## Load Data for Model Runs

```{r, echo=FALSE}
# Single site HC - stop @ 2014

# Source Functions
  source('0_Function_library/hindcasting_get_data_site.R') #check correct script sourced
  source('0_Function_library/model_calibration_plug_n_play_site.R')

# Get model
my_models <- c("wtrtemp_min_and_GDD_1site_RY") #,"RW_obs_1site", "DLM_1site","wtrtemp_min_and_GDD_1site", "wtrtemp_min_and_GDD_1site_RY")

model_name = my_models
model=paste0("4.1_JAGS_models/",model_name, '.R') #Do not edit

# Read in data for model
hindcast_data <- get_hindcast_data(model_name = model_name,
                                   year = 2015)

# JAGS Plug-Ins => initial conditions, priors, data, etc.
jags_plug_ins <- jags_plug_ins(model_name = model_name)

# Run model (no edits, unless you want to change # of iterations)
j.model   <- jags.model (file = model,
                         data = jags_plug_ins$data.model,
                         inits = jags_plug_ins$init.model,
                         n.chains = 3)
# jags
jags.out <- run.jags(model = model,
                     data = jags_plug_ins$data.model,
                     adapt =  5000, # not the same as thinning - Mary: Thinned to 7,500 samples for hindcasting and model assessment
                     burnin =  10000,
                     sample = 50000, #50000
                     n.chains = 3,
                     inits=jags_plug_ins$init.model,
                     monitor = jags_plug_ins$variable.namesout.model)

#convert to an MCMC list to calculate cross-correlation later
jags.out.mcmc <- as.mcmc.list(jags.out)


#6) Save output for calibration assessment

#save predicted states
Nmc = 10000
out <- as.matrix(jags.out.mcmc) # Window to thin, burn-in,
srow <- sample.int(nrow(out),Nmc,replace=TRUE)
mus <- out[srow,grep("mu",colnames(out))]
betas <- out[srow, grep("beta",colnames(out))]
taus <- out[srow, grep("tau",colnames(out))]
params_init <- cbind(betas, taus)
```


### wtrtemp_min_and_GDD_1site_RY Model
```{r}
mod1 = "model{

    #### Data Model
    #this fits the latent Gloeo to your observed Gloeo
    #run this on logged data
    for(t in 1:max(season_weeks)){

    y[t] ~ dnorm(mu[t],tau_obs)

    #gap-filling model for covariates
    covar1[t]~dnorm(week_avg1[t],tau_C1_proc)
    covar2[t]~dnorm(week_avg2[t],tau_C2_proc)

  }

  #### Process Model
  for(t in 2:max(season_weeks)){
    mu[t]~dnorm(lambda[t],tau_proc)
    lambda[t] <- beta1  + beta2*mu[t-1] + beta3*covar1[t] + beta4*covar2[t] + beta5*covar2[t]^2 + yr[year_no[t]]

  }

  #### Initial Conditions
  mu[1] ~ dnorm(x_ic,tau_ic)
  #lambda[1] ~ dnorm(x_ic, tau_ic) # might not need now?

  ### Random Year Effect
  for(k in 1:totYr){
    yr[k] ~ dnorm(0, tau_yr) # Centered on 0 or beta1
  }

  #### Priors
  beta1 ~ dnorm(beta.m1,beta.v1)
  beta2 ~ dnorm(beta.m2,beta.v2)
  beta3 ~ dnorm(beta.m3,beta.v3)
  beta4 ~ dnorm(beta.m4,beta.v4)
  beta5 ~ dnorm(beta.m5,beta.v5)

  tau_C1_proc ~ dgamma(0.01,0.01)
  tau_C2_proc ~ dgamma(0.01,0.01)

  tau_obs ~ dgamma(a_obs,r_obs)
  tau_proc ~ dgamma(a_proc,r_proc)
  tau_yr ~ dgamma(0.01,0.01)


}"
```

### Test coda jags function
```{r}
# Single site HC - stop @ 2014

# Source Functions
  source('0_Function_library/hindcasting_get_data_site.R') #check correct script sourced
  source('0_Function_library/model_hindcasting_plug_n_play_site.R')

# Get model
my_models <- c("wtrtemp_min_and_GDD_1site_RY") #,"RW_obs_1site", "DLM_1site","wtrtemp_min_and_GDD_1site", "wtrtemp_min_and_GDD_1site_RY")

model_name = my_models
model=paste0("4.1_JAGS_models/",model_name, '.R') #Do not edit

# Read in data for model
hindcast_data <- get_hindcast_data(model_name = model_name,
                                   year = 2015)
source('0_Function_library/model_calibration_get_data_site_longformat.R') #check correct script sourced
  source('0_Function_library/model_calibration_plug_n_play_site.R')

cal_data <- get_calibration_data(model_name)

# JAGS Plug-Ins => initial conditions, priors, data, etc.
jags_plug_ins <- jags_plug_ins(model_name = model_name)

# Run model (no edits, unless you want to change # of iterations)
j.model   <- jags.model (file = model,
                         data = jags_plug_ins$data.model,
                         inits = jags_plug_ins$init.model,
                         n.chains = 3)
# jags
jags.out <- coda.samples(model = j.model,
                         variable.names = c("tau_proc", "beta1", "beta2","beta3","beta4","beta5", "mu", "tau_obs", "tau_C1_proc", "tau_C2_proc","covar1","covar2"),
                         n.iter = 50000)

plot(jags.out)

#,"covar1","covar2"

burnin = 10000
jags.burn <- window(jags.out,start=burnin)  ## remove burn-in and thin to 7500 (currently 7143 with thin = 7), I think can thin later, leave out here
#jags.thin = window(jags.burn,thin=10)


jags.out <- run.jags(model = model,
                     data = jags_plug_ins$data.model,
                     adapt =  5000, # not the same as thinning - Mary: Thinned to 7,500 samples for hindcasting and model assessment
                     burnin =  10000,
                     sample = 50000, #50000
                     n.chains = 3,
                     inits=jags_plug_ins$init.model,
                     monitor = jags_plug_ins$variable.namesout.model)

#convert to an MCMC list to calculate cross-correlation later
jags.out.mcmc <- as.mcmc.list(jags.out)


#6) Save output for calibration assessment

#save predicted states
Nmc = 10000
out <- as.matrix(jags.out.mcmc) # Window to thin, burn-in,
srow <- sample.int(nrow(out),Nmc,replace=TRUE)
mus <- out[srow,grep("mu",colnames(out))]
betas <- out[srow, grep("beta",colnames(out))]
taus <- out[srow, grep("tau",colnames(out))]
params_init <- cbind(betas, taus)

```

### Fill in missing data for covariates
- calculate weekly mean of min water temp @ all sites EXCEPT fichter
- cross-site variance for weekly differences
```{r}

# Read in covariate data
wtrtempmin_allsites <- read_csv("./00_Data_files/Bayesian_model_input_data/wtrtemp_min_AllSites.csv")

wtr_weeklymean <- rowMeans(wtrtempmin_allsites[,-c(1:2,5)], na.rm = T) # drop year, season week & Fichter data

# Fill NA - Bring back season week
wtr_weeklymean_v2 <- cbind(wtrtempmin_allsites$year, wtrtempmin_allsites$season_week, wtr_weeklymean)

colnames(wtr_weeklymean_v2)[1:2] <- c("year", "season_week")

sum(is.na(wtr_weeklymean_v2)) # 16 missing values mostly season weeks 19 & 20 but also 1 &2

missing <- which(is.na(wtr_weeklymean_v2[,3]))

# works but throws error so watch if knitting
for(i in 1:length(wtr_weeklymean_v2)) {
  if(is.na(wtr_weeklymean_v2[i,3]) & wtr_weeklymean_v2[i,2] %in% c(19:20)) {
    wtr_weeklymean_v2[i,3] <- wtr_weeklymean_v2[i-1,3]
    
  } else if (is.na(wtr_weeklymean_v2[i,3]) & wtr_weeklymean_v2[i,2] %in% c(2))
    wtr_weeklymean_v2[i,3] <- wtr_weeklymean_v2[i+1,3]
  
  
  else{
    
    if(is.na(wtr_weeklymean_v2[i,3]) & wtr_weeklymean_v2[i,2] %in% c(1)) {
    wtr_weeklymean_v2[i,3] <- wtr_weeklymean_v2[i+1,3]
  }
 }
}

write.csv(wtr_weeklymean_v2, "./00_Data_files/Bayesian_model_input_data/wtrtemp_min_allsites_prior.csv", row.names = F)

# Look at site variance
var(wtrtempmin_allsites[,c(3:4,6)], na.rm = T) #drop Fichter

# Calculate weekly avg for all sites except Fichter to use as prior
gdd_allsites <- read_csv("./00_Data_files/Bayesian_model_input_data/GDD_AllSites.csv")

gdd_weeklymean <- rowMeans(gdd_allsites[,-c(1:2,5)], na.rm = T) # drop year, season week & Fichter data

# Fill NA - Bring back season week
gdd_weeklymean_v2 <- cbind(gdd_allsites$year, gdd_allsites$season_week, gdd_weeklymean)

colnames(gdd_weeklymean_v2)[1:2] <- c("year", "season_week")

sum(is.na(gdd_weeklymean_v2)) # 16 missing values mostly season weeks 19 & 20 but also 1 &2

missing <- which(is.na(gdd_weeklymean_v2[,3]))

for(i in 1:length(gdd_weeklymean_v2[,3])) {
  if(is.na(gdd_weeklymean_v2[i,3]) & gdd_weeklymean_v2[i,2] %in% c(19:20)) {
    gdd_weeklymean_v2[i,3] <- gdd_weeklymean_v2[i-1,3]
    
  } else if (is.na(gdd_weeklymean_v2[i,3]) & gdd_weeklymean_v2[i,2] %in% c(2))
    gdd_weeklymean_v2[i,3] <- gdd_weeklymean_v2[i+1,3]
  
  
  else{
    
    if(is.na(gdd_weeklymean_v2[i,3]) & gdd_weeklymean_v2[i,2] %in% c(1)) {
    gdd_weeklymean_v2[i,3] <- gdd_weeklymean_v2[i+1,3]
  }
 }
}

write.csv(gdd_weeklymean_v2, "./00_Data_files/Bayesian_model_input_data/gdd_allsites_prior.csv", row.names = F)


```


### Posterior Diagnostics

As noted earlier, we're going to take the fitting of this model to the data as a given, and work with the posterior distributions, but if you are unsure about how this is done please go back and look at Exercises 5 and 6 and the Chapter 6 activities. Here are the basic diagnostics showing the posterior estimates of the parameters and states.


## JB Posterior

```{r}
y = cal_data$y
time = cal_data$season_weeks # for full time series
#time = date # for 2013
time.rng = c(1,length(time))       ## adjust to zoom in and out
#time.rng = c(321,480)              ## zoom in on 2013: 81-100, 2015-2016: 121-160, 141-160 - 2016
#out <- as.matrix(jags.out)         ## convert from coda to matrix  
mu.cols <- grep("^mu",colnames(out)) # out ## grab all columns that start with the letter mu

ci <- apply(out[,mu.cols],2,quantile,c(0.025,0.5,0.975)) ## model was fit on log scale but keep in log scale

plot(time,ci[2,],type='n',ylim=range(ci,na.rm=TRUE),ylab="LN Gloeo", xlab = "Week", xlim=time[time.rng], main = "1site + RY Herrick Cove") #, xaxt = "n") #, xaxt = "n"#log='y', don't need on log scale
if(diff(time.rng) < 100){ 
  axis.Date(1, at=seq(time[time.rng[1]],time[time.rng[2]],by='week'), format = "%Y-%m-%d", labels = T)
}
ecoforecastR::ciEnvelope(time,ci[1,],ci[3,],col=ecoforecastR::col.alpha(5,0.75))
#points(time,y,pch=19,cex=1) #0.5, pch="+",
#lines(time,y)
lines(time, ci[2,], lty = 2)
#lines(time, ci[3,], lty = 2)
points(time, ci[2,], col = 5, pch=19, cex = 1)
points(time, ci[2,], cex = 1)

```

## Forward Simulation

```{r, echo=FALSE}
### settings
s <- 6             ## Focal site for forward simulation
Nmc = 7500         ## set number of Monte Carlo draws 1000
ylim = range(ci,na.rm=TRUE)  ## set Y range on plot
N.cols <- c("black","red","green","blue","orange") ## set colors
trans <- 0.8       ## set transparency
time = 1:140    ## total time
time1 = 1:120       ## calibration period
time2 = 121:140   ## forecast period
```

Before we get started with prediction, let's focus in on a single site to make the task a bit simpler. Because we'll want to display this site for all the different model runs we do let's create a simple function to encapsulate making the plot rather than cluttering our code with constant cut-and-paste redundancy.

```{r}
plot.run <- function(){
 # sel = seq(s,ncol(ci),by=NS) # add a sequence to avoid overplotting
  plot(time,time,type='n',ylim=ylim,ylab="LN Gloeo")
  ecoforecastR::ciEnvelope(time1,ci[1,],ci[3,],col=col.alpha(5,0.6))
  lines(time1,ci[2,],col="blue")
  points(time1,ci[2,])
}
```

```{r,echo=FALSE}
#ci <- apply(out[,mu.cols],2,quantile,c(0.025,0.5,0.975)) 
plot.run()
```

### Forward simulation Forecast Function
```{r}

## initial conditions
IC <- as.matrix(mus)

forecastN <- function(IC,params,n=Nmc){

  forecast_time <- 20 # 20 season weeks = 1 year gloeo data

  proc.model <- matrix(NA, n, forecast_time) # storage
  out <- matrix(NA, n, forecast_time) # storage

  gloeo_prev <- IC          ## initialize

  for(t in 1:length(time2)){ # start for last 2 years
    #covar model
      #  covar1 <- covar_hindcast$covar1[t] # hindcasted covariates
      #  covar2 <- covar_hindcast$covar2[t]
    covar1 <- gloeo$Temp2[t]
    covar2 <- gloeo$GDD2[t]
    #process model
   gloeo_temp = params$beta1 + params$beta2*gloeo_prev + params$beta3*covar1 + params$beta4*covar2 + params$beta5*covar2^2 
    
   proc.model[,t] = rnorm(n,gloeo_temp,params$sd_proc) 
    
   #data model
    out[,t] = rnorm(n,proc.model[,t],params$sd_obs)      ## predict next step with data model
    gloeo_prev <- out[,t]                                 ## update IC
  }
  return(out)
  }

```


Our next goal in making a forecast is to run the model forward under it's 'default' values, which we'll take as the mean of the various inputs. To help us out let's make another function that encapsulates our model and generalizes it for the different cases we'll need. This code is directly analogous to the process model in the earlier JAGS code, and we don't need to replicate the data or parameter models.

To make a deterministic prediction, we'd then want to calculate the means of all of our different inputs and pass these means to our model. Since there run is deterministic, we only need to run it once (n=1)

### Deterministic prediction
```{r}
posteriors = out
n = 1 # just run once

# Deterministic = parameters set to mean
params.det <- list(sd_obs = 0, sd_proc = 0, beta1 = mean(posteriors[,grep("beta1",colnames(posteriors))],na.rm = TRUE),
                     beta2 = mean(posteriors[,grep("beta2",colnames(posteriors))],na.rm = TRUE), beta3 = mean(posteriors[,grep("beta3",colnames(posteriors))],na.rm = TRUE),
                     beta4 = mean(posteriors[,grep("beta4",colnames(posteriors))],na.rm = TRUE),beta5 = mean(posteriors[,grep("beta5",colnames(posteriors))],na.rm = TRUE),sd_C1 = 0, sd_C2 = 0)

  ## storage
  #set up output matrices

# Testing for loop
  for(t in 1:length(time2)){ # start for last 2 years
    #covar model
      #  covar1 <- covar_hindcast$covar1[t] # hindcasted covariates
      #  covar2 <- covar_hindcast$covar2[t]
    covar1 <- gloeo$Temp2[t]
    covar2 <- gloeo$GDD2[t]
    #process model
   gloeo_temp = params$beta1 + params$beta2*gloeo_prev + params$beta3*covar1 + params$beta4*covar2 + params$beta5*covar2^2 
    
   proc.model[t] = rnorm(n,gloeo_temp,params$sd_proc) 
    
    
   #data model
    out[t] = rnorm(n,proc.model[t],params$sd_obs)      ## predict next step with data model
    gloeo_prev <- out[t]                                 ## update IC
  }
 
N.det <- forecastN(IC = mean(IC[,"mu[120]"]),  ## mean of IC
                 params = params.det,
                 n=1)
  
## Plot run
plot.run()
lines(time2,N.det,col="purple",lwd=3)

write.csv(N.det,file=file.path(paste("./5_Model_output/5.2_Hindcasting/",paste0(model_name,'_det.prediction_','HC','.csv'))),row.names = FALSE)

```


### Mary model Deterministic prediction

```{r}

posteriors = out
Nmc = 7500
prow = sample.int(nrow(out),Nmc,replace=TRUE)
num_draws = prow


params.det <- list(sd_obs = 0, sd_proc = 0, beta1 = mean(posteriors[,grep("beta1",colnames(posteriors))],na.rm = TRUE),
                     beta2 = mean(posteriors[,grep("beta2",colnames(posteriors))],na.rm = TRUE), beta3 = mean(posteriors[,grep("beta3",colnames(posteriors))],na.rm = TRUE),
                     beta4 = mean(posteriors[,grep("beta4",colnames(posteriors))],na.rm = TRUE),beta5 = mean(posteriors[,grep("beta5",colnames(posteriors))],na.rm = TRUE),sd_C1 = 0, sd_C2 = 0)

#pull a whole row, thin 1000, run 1:1000 

#run deterministic hindcast
covar.hindcast.det <- NA
season_weeks <- 1:140

for(t in 1:length(season_weeks)){
det.prediction <- run_hindcast(model_name = model_name,
                                     params = params.det,
                                     Nmc = 1,
                                     IC = mean(IC[,"mu[120]"]), #last time point before hindcast # IC = mean(IC), 
                                     season_weeks = season_weeks[t]) #, covar_hindcast = covar.hindcast.det
}
      
## initial conditions
IC <- as.matrix(mus)

IC = rnorm(Nmc,-5,sqrt(1/100)) # Mary restarted new year

write.csv(det.prediction,file=file.path(paste("./5_Model_output/5.2_Hindcasting/",paste0(model_name,'_det.prediction_',yrs[j],'_',wks[k],'.csv'))),row.names = FALSE)

## Plot run
plot.run()
lines(time2,det.prediction,col="purple",lwd=3)
```

# Monte Carlo Error Propagation

For each source of uncertainty we'll be exploring the Monte Carlo approach to error propagation. Let's start with the initial condition uncertainty. For this set of runs we'll want to continue to hold all the parameters and drivers at their means, and keep the process error off, but we'll vary the initial conditions. Specifically, we'll run the model `r Nmc` times, and each time we run the model we'll start from a different initial condition. For this analysis the distribution of initial conditions that we want to sample from is just the posterior distribution of the population size at our focal site in the last year of the calibration period, `N[6,30]`. Because we know that posterior distributions often have covariances among parameters, we're going to sample the _row numbers_ in the MCMC output rather than sample the posterior state directly. We'll save these row numbers and use the same rows when we later sample the model parameters, process error, and random effects as well. While we had the benefit of the posterior state estimate here, in other contexts the initial condition uncertainty might be derived directly from field sampled data, or (for unmeasured sites) may require another statistical model to make a _spatial_ prediction of the state of the system to new locations. If we're particularly data limited we may have to rely on Bayesian priors to inform the initial conditions (e.g. through expert elicitation).

### Initial Condition uncertainty
```{r}
## sample parameter rows from previous analysis
#prow = sample.int(nrow(params),Nmc,replace=TRUE)

Nmc = 7500

prow = sample.int(nrow(params_init),Nmc,replace=TRUE) #sample parameter rows

forecast_time <- 20



N.I <- forecastN(IC = IC[prow,"mu[120]"],  ## sample IC
                 params = params.det,
                 n=Nmc)

## Plot run
plot.run()
N.I.ci = apply(N.I,2,quantile,c(0.025,0.5,0.975))
ecoforecastR::ciEnvelope(time2,N.I.ci[1,],N.I.ci[3,],col=col.alpha(N.cols[1],trans))
lines(time2,N.I.ci[2,],lwd=0.5)

write.csv(N.I,file=file.path(paste("./5_Model_output/5.2_Hindcasting/",paste0(model_name,'_hindcast.IC_','HC','.csv'))),row.names = FALSE)

```

**Question 1:** Describe the pattern to the initial condition uncertainty and compare that to your expectations (e.g. Table 1).

### Parameter uncertainty

Next, let's add parameter uncertainty to our forecast. To do this we'll want to sample the **joint** parameter posterior distribution using the row numbers as before. In this forecast we're still running `r Nmc` model runs, but now each run get's its own unique parameters _and_ its own unique initial conditions. We expect the total uncertainty to increase, so we can layer the run with both uncertainties on top of the run with just initial conditions to get a feel for how much more uncertainty is added when we incorporate parameter uncertainty.

One thing that is important to note about the parameter uncertainty propagation is that it relies on having meaningful estimates of parameter uncertainty. That's not to say that forecasts can't be done with priors (e.g. through literature synthesis or expert elicitation), but forecasting what you expect to happen in a real system is definitely different than slapping an arbitrary uniform prior on default parameters (e.g. +/- 20%).

```{r}
prow = sample.int(nrow(params_init),Nmc,replace=TRUE) #sample parameter rows

params.ip <- list(sd_obs = 0, sd_proc = 0, beta1 = posteriors[prow,"beta1"],
                     beta2 = posteriors[prow,"beta2"], beta3 = posteriors[prow,"beta3"], beta4 = posteriors[prow,"beta4"],
                     beta5 = posteriors[prow,"beta5"],sd_C1 = 0, sd_C2 = 0)
 
N.IP <- forecastN(IC = IC[prow,"mu[120]"],
                  params = params.ip,
                  n = Nmc)


## Plot run
plot.run()
N.IP.ci = apply(N.IP,2,quantile,c(0.025,0.5,0.975))
ecoforecastR::ciEnvelope(time2,N.IP.ci[1,],N.IP.ci[3,],col=col.alpha(N.cols[2],trans))
ecoforecastR::ciEnvelope(time2,N.I.ci[1,],N.I.ci[3,],col=col.alpha(N.cols[1],trans))
lines(time2,N.I.ci[2,],lwd=0.5)

write.csv(N.IP,file=file.path(paste("./5_Model_output/5.2_Hindcasting/",paste0(model_name,'_hindcast.IC.Pa_','HC','.csv'))),row.names = FALSE)

```

**Question 2:** 
A) Describe the pattern and relative magnitude of the parameter uncertainty and compare this to your expectations. 
B) Using the existing model and Monte Carlo approach, describe how you might partition the contribution of each parameter (r, Kg, alpha, beta) to the forecast uncertainty.

### Driver uncertainty

Within our forecast precipitation is the primary extrinsic driver, and thus to make a forecast of our population we need to be able to make a forecast of this driver as well. Here we're going to rely on a `NE` member ensemble of precipitation forecasts, and as you can see the uncertainty in this forecasts grows with time.

```{r,echo=FALSE}
# plot(time2,ppt_ensemble[1,],type='n',ylim=range(ppt_ensemble),xlab="time",ylab="precipitation (mm)")
# for(i in 1:NE){
#   lines(time2,ppt_ensemble[i,],lwd=0.5,col="grey")
# }
```

This example of having a smaller number of driver ensembles is not uncommon, and we can handle this easily by resampling _with replacement_ the ensemble members (again, represented as row numbers) the same number of times as our overall ensembles. Thus, while every run may have a unique initial condition and parameter combination, many runs share the same drivers.

When driving ecological forecasts with other forecast outputs (weather, land use, economics, etc), it is important to remember that we can only resample **predictions** or we can sample ensemble members **within a projection**, but we CANNOT resample **across projections**. For example, if I have a given climate scenario (e.g. RCP4.5), I can resample over what different climate models predict, but I cannot resample or average across the different scenarios (e.g. RCP4.5 and 8.5), because those scenarios themselves do not represent random draws from the distribution of possible futures.


**Currently same as parameter uncertainty** - need to include covariate ensembles
```{r}

params.ipd <- list(sd_obs = 0, sd_proc = 0, beta1 = posteriors[prow,"beta1"],
              beta2 = posteriors[prow,"beta2"], beta3 = posteriors[prow,"beta3"], beta4 = posteriors[prow,"beta4"], 
              beta5 = posteriors[prow,"beta5"],
              sd_C1 = 1/sqrt(posteriors[prow,"tau_C1_proc"]), sd_C1 = 1/sqrt(posteriors[prow,"tau_C2_proc"]))


N.IPD <- forecastN(IC = IC[prow,"mu[120]"],
                  params = params.ipd,
                  n = Nmc)

## Plot run
plot.run()
N.IPD.ci = apply(N.IPD,2,quantile,c(0.025,0.5,0.975))
ecoforecastR::ciEnvelope(time2,N.IPD.ci[1,],N.IPD.ci[3,],col=col.alpha(N.cols[3],trans)) # driver
ecoforecastR::ciEnvelope(time2,N.IP.ci[1,],N.IP.ci[3,],col=col.alpha(N.cols[2],trans)) # parameter
ecoforecastR::ciEnvelope(time2,N.I.ci[1,],N.I.ci[3,],col=col.alpha(N.cols[1],trans)) # IC
lines(time2,N.I.ci[2,],lwd=0.5)

write.csv(N.IPD,file=file.path(paste("./5_Model_output/5.2_Hindcasting/",paste0(model_name,'_hindcast.IC.Pa.D_','HC','.csv'))),row.names = FALSE)

```

**Question 3:** As before, describe the pattern of how driver uncertainty contributes to the forecast and compare that to our first principles expectations.

### Process uncertainty

Adding process error to the model looks similar to both older _stochastic_ modeling approaches and to the _residual_ error in statistical models. However, it is subtly distinct in a few important ways. First, unlike theoretical stochastic models, process error is estimated explicitly by fitting models to data. Second, unlike residual error, when we estimate process error we partition it from the observation error. Thus the process error represents things in the process that the model is unable to capture (which will always occur because all models are approximations). The distinction between process and observation error is important because observation error only affects the current observation, but process error propagates into the future.

```{r}
## process error samples
params.ipde <- list(sd_obs = 0, sd_proc = 1/sqrt(posteriors[prow,"tau_proc"]), ## convert from precision to standard deviation
               beta1 = posteriors[prow,"beta1"],beta2 = posteriors[prow,"beta2"], beta3 = posteriors[prow,"beta3"], 
               beta4 = posteriors[prow,"beta4"], beta5 = posteriors[prow,"beta5"],
               sd_C1 = 1/sqrt(posteriors[prow,"tau_C1_proc"]), sd_C1 = 1/sqrt(posteriors[prow,"tau_C2_proc"]))


N.IPDE <- forecastN(IC = IC[prow,"mu[120]"],
                  params = params.ipde,
                  n = Nmc)

## Plot run
plot.run()
N.IPDE.ci = apply(N.IPDE,2,quantile,c(0.025,0.5,0.975))
ecoforecastR::ciEnvelope(time2,N.IPDE.ci[1,],N.IPDE.ci[3,],col=col.alpha(N.cols[4],trans))
ecoforecastR::ciEnvelope(time2,N.IPD.ci[1,],N.IPD.ci[3,],col=col.alpha(N.cols[3],trans))
ecoforecastR::ciEnvelope(time2,N.IP.ci[1,],N.IP.ci[3,],col=col.alpha(N.cols[2],trans))
ecoforecastR::ciEnvelope(time2,N.I.ci[1,],N.I.ci[3,],col=col.alpha(N.cols[1],trans))
lines(time2,N.I.ci[2,],lwd=0.5)

write.csv(N.IPDE,file=file.path(paste("./5_Model_output/5.2_Hindcasting/",paste0(model_name,'_hindcast.IC.Pa.D.P_','HC','.csv'))),row.names = FALSE)

```

### Observation Uncertainty
```{r}
params.ipdeo <- list(sd_obs = 1/sqrt(posteriors[prow,"tau_obs"]), sd_proc = 1/sqrt(posteriors[prow,"tau_proc"]), 
                     beta1 = posteriors[prow,"beta1"],beta2 = posteriors[prow,"beta2"], beta3 = posteriors[prow,"beta3"], 
                     beta4 = posteriors[prow,"beta4"], beta5 = posteriors[prow,"beta5"],
                     sd_C1 = 1/sqrt(posteriors[prow,"tau_C1_proc"]), sd_C2 = 1/sqrt(posteriors[prow,"tau_C2_proc"]))


N.IPDEO <- forecastN(IC = IC[prow,"mu[120]"],
                  params = params.ipdeo,
                  n = Nmc)

## Plot run
plot.run()
N.IPDEO.ci = apply(N.IPDEO,2,quantile,c(0.025,0.5,0.975))
ecoforecastR::ciEnvelope(time2,N.IPDEO[1,],N.IPDEO[3,],col=col.alpha(N.cols[5],trans))
ecoforecastR::ciEnvelope(time2,N.IPDE.ci[1,],N.IPDE.ci[3,],col=col.alpha(N.cols[4],trans))
ecoforecastR::ciEnvelope(time2,N.IPD.ci[1,],N.IPD.ci[3,],col=col.alpha(N.cols[3],trans))
ecoforecastR::ciEnvelope(time2,N.IP.ci[1,],N.IP.ci[3,],col=col.alpha(N.cols[2],trans))
ecoforecastR::ciEnvelope(time2,N.I.ci[1,],N.I.ci[3,],col=col.alpha(N.cols[1],trans))
lines(time2,N.I.ci[2,],lwd=0.5)

write.csv(N.IPDEO,file=file.path(paste("./5_Model_output/5.2_Hindcasting/",paste0(model_name,'_hindcast.IC.Pa.D.P.O_','HC','.csv'))),row.names = FALSE)
```


**Question 4:** Why might the relative contribution of the process error decline with time? Does this match expectations?

### Random Effect uncertainty

The last figure above represent our best forecast if we're making an **in-sample** prediction (i.e. forecasting one of the sites that we have previously studied). However, if we are making an **out-of-sample** prediction to a new site then there is an additional source of uncertainty that needs to be considered -- the unexplained site-to-site variability being captured by the random effect. When we're making an in-sample prediction we use the posterior $\alpha$ associated with that specific site, but for a new site we need to move up a level in the hierarchical model and sample new alpha values using the across-site precision, $\tau_{site}$. While the out-of-sample forecast is going to have greater uncertainty (which makes intuitive sense and is a desirable feature), there's no guarantee that the in-sample prediction is within the predictive interval of the out-of-sample prediction because the focal site might have been above- or below-average relative to the across-site mean.

```{r}
## Random effect samples
tau.mc <- 1/sqrt(params[prow,"tau_site"]) ## convert from precision to std deviation
aNew.mc <- rnorm(Nmc,0,tau.mc)            ## draw out-of-sample predictions of alpha at a new site

N.IPDEA <- forecastN(IC=IC[prow,"N[6,30]"],    ## sample IC
                   ppt=ppt_ensemble[drow,],    ## Sample drivers
                   r=params[prow,"r_global"],  ## sample parameters
                   Kg=params[prow,"K_global"],
                   beta=params[prow,"beta"],
                   alpha=aNew.mc,              ## sample random effect
                   Q=Qmc,                      ## process error
                   n=Nmc)

## Plot run
plot.run()
N.IPDEA.ci = apply(N.IPDEA,2,quantile,c(0.025,0.5,0.975))
ecoforecastR::ciEnvelope(time2,N.IPDEA.ci[1,],N.IPDEA.ci[3,],col=col.alpha(N.cols[5],trans))
ecoforecastR::ciEnvelope(time2,N.IPDE.ci[1,],N.IPDE.ci[3,],col=col.alpha(N.cols[4],trans))
ecoforecastR::ciEnvelope(time2,N.IPD.ci[1,],N.IPD.ci[3,],col=col.alpha(N.cols[3],trans))
ecoforecastR::ciEnvelope(time2,N.IP.ci[1,],N.IP.ci[3,],col=col.alpha(N.cols[2],trans))
ecoforecastR::ciEnvelope(time2,N.I.ci[1,],N.I.ci[3,],col=col.alpha(N.cols[1],trans))
lines(time2,N.I.ci[2,],lwd=0.5)
```

**Question 5:** The last step added random effects, but this uncertainty could be further partitioned into two parts: the ecological site-to-site variability in the population processes, and the parameter uncertainty about the site-to-site variance. To partition these out repeat this analysis setting `tau.mc` to its posterior mean. How does this change your results? How much of the random effect uncertainty is due to ecological variability versus uncertainty about `tau.mc`?

```{r}
## Random effect samples with ecological variability vs. tau.mc
tau.mc <- 1/sqrt(params[prow,"tau_site"]) ## convert from precision to std deviation
tau.mc <- param.mean["tau_site"]
aNew.mc <- rnorm(Nmc,0,tau.mc)            ## draw out-of-sample predictions of alpha at a new site

N.IPDEA <- forecastN(IC=IC[prow,"N[6,30]"],    ## sample IC
                   ppt=ppt_ensemble[drow,],    ## Sample drivers
                   r=params[prow,"r_global"],  ## sample parameters
                   Kg=params[prow,"K_global"],
                   beta=params[prow,"beta"],
                   alpha=aNew.mc,              ## sample random effect
                   Q=Qmc,                      ## process error
                   n=Nmc)

## Plot run
plot.run()
N.IPDEA.ci = apply(N.IPDEA,2,quantile,c(0.025,0.5,0.975))
ecoforecastR::ciEnvelope(time2,N.IPDEA.ci[1,],N.IPDEA.ci[3,],col=col.alpha(N.cols[5],trans))
ecoforecastR::ciEnvelope(time2,N.IPDE.ci[1,],N.IPDE.ci[3,],col=col.alpha(N.cols[4],trans))
ecoforecastR::ciEnvelope(time2,N.IPD.ci[1,],N.IPD.ci[3,],col=col.alpha(N.cols[3],trans))
ecoforecastR::ciEnvelope(time2,N.IP.ci[1,],N.IP.ci[3,],col=col.alpha(N.cols[2],trans))
ecoforecastR::ciEnvelope(time2,N.I.ci[1,],N.I.ci[3,],col=col.alpha(N.cols[1],trans))
lines(time2,N.I.ci[2,],lwd=0.5)
```


# Uncertainty Analysis

Our final analysis is focused on quantifying the relative contributions of each of the 5 uncertainty terms to the overall predictive variance, and how that partitioning changes with time. To do this we will calculate the variances, because unlike predictive intervals or standard errors variances combine additively.

```{r}
### calculation of variances
varI     <- apply(N.I,2,var)
varIP    <- apply(N.IP,2,var)
varIPD   <- apply(N.IPD,2,var)
varIPDE  <- apply(N.IPDE,2,var)
#varIPDEA <- apply(N.IPDEA,2,var) #dropped
#varMat   <- rbind(varI,varIP,varIPD,varIPDE,varIPDEA)
varMat   <- rbind(varI,varIP,varIPD,varIPDE)

## out-of-sample stacked area plot
V.pred.rel <- apply(varMat,2,function(x) {x/max(x)})
plot(time2,V.pred.rel[1,],ylim=c(0,1),type='n',main="Relative Variance: Out-of-Sample",ylab="Proportion of Variance",xlab="time")
ciEnvelope(time2,rep(0,ncol(V.pred.rel)),V.pred.rel[1,],col=N.cols[1])
ciEnvelope(time2,V.pred.rel[1,],V.pred.rel[2,],col=N.cols[2])
ciEnvelope(time2,V.pred.rel[2,],V.pred.rel[3,],col=N.cols[3])
ciEnvelope(time2,V.pred.rel[3,],V.pred.rel[4,],col=N.cols[4])
ciEnvelope(time2,V.pred.rel[4,],V.pred.rel[5,],col=N.cols[5])
legend("topleft",legend=c("RandomEffect","Process","Driver","Parameter","InitCond"),col=rev(N.cols),lty=1,lwd=5)

## in-sample stacked area plot
#V.pred.rel.in <- apply(varMat[-5,],2,function(x) {x/max(x)})
V.pred.rel.in <- apply(varMat,2,function(x) {x/max(x)})

plot(time2,V.pred.rel.in[1,],ylim=c(0,1),type='n',main="Relative Variance: In-Sample",ylab="Proportion of Variance",xlab="time")
ciEnvelope(time2,rep(0,ncol(V.pred.rel.in)),V.pred.rel.in[1,],col=N.cols[1])
ciEnvelope(time2,V.pred.rel.in[1,],V.pred.rel.in[2,],col=N.cols[2])
ciEnvelope(time2,V.pred.rel.in[2,],V.pred.rel.in[3,],col=N.cols[3])
ciEnvelope(time2,V.pred.rel.in[3,],V.pred.rel.in[4,],col=N.cols[4])
legend("topleft",legend=c("Process","Driver","Parameter","InitCond"),col=rev(N.cols[-5]),lty=1,lwd=5)
```


**Question 6:**  Based on results, where would you focus your research efforts next if your aim was to improve out-of-sample predictions? What about improving in-sample predictions? What is the current balance between process and parameter error and what does that tell us about the potential value of increasing model complexity?


